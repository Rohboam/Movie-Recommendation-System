# -*- coding: utf-8 -*-
"""IR_FinalProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e9ckv4vmUg0onjxyIXKv2BXD4hdsmeiI
"""

from google.colab import drive
drive.mount("/content/drive")

cd 'drive/My Drive'

import pandas as pd
data= pd.read_csv("wiki_movie_plots_deduped.csv")
data.head()

data.shape

import numpy as np
np.unique(data['Origin/Ethnicity'])

data2 = data.loc[(data['Origin/Ethnicity']=='American')&(data['Release Year']>2015)]
len(data2)

# test = data2.copy()

my_data= pd.DataFrame(data2)

my_data.tail()

finaldata = my_data[["Title","Plot"]]

test = finaldata.copy()

finaldata = finaldata.set_index('Title')
finaldata.head(10)

test.shape

temp = pd.DataFrame({"Title": ['Query'],
                     "Plot": ['aaaaaaa']})

test2 = test.append(temp)

# pd.concat([test, temp])

test2.shape

test2.query('Title == "Query"')

test2.tail()

test2 = test2.set_index('Title')

test2["Plot"]["Query"]

finaldata["Plot"]["Deadpool"]

import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
nltk.download('stopwords')

from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords

lemmatizer = WordNetLemmatizer()

def preprocess_sentences(text):
  text = text.lower()
  words = nltk.wordpunct_tokenize(text)
  my_sent=[lemmatizer.lemmatize(word) for word in words if word not in stopwords.words('english')]
  finalsent = ' '.join(my_sent)

  finalsent = finalsent.replace("n't","not")
  finalsent = finalsent.replace("'m","am")
  finalsent = finalsent.replace("'s","is")
  finalsent = finalsent.replace("'re","are")
  finalsent = finalsent.replace("'ll","will")
  finalsent = finalsent.replace("'ve","have")
  finalsent = finalsent.replace("'d","would")

  return finalsent

finaldata["new_plot"] = finaldata["Plot"].apply(preprocess_sentences)
finaldata.head()

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer()
tfidf_movieid = tfidf.fit_transform((finaldata["new_plot"]))

# print(tfidf_movieid)
tfidf.get_feature_names_out()

#Finding cosine Similarity
from sklearn.metrics.pairwise import cosine_similarity
similarity = cosine_similarity(tfidf_movieid, tfidf_movieid)

# similarity2 = cosine_similarity(query, tfidf_movieid)

tfidf.get_feature_names_out()

indices = pd.Series(finaldata.index)

def recommendations(title, cosine_sim= similarity):
  try:
    index= indices[indices == title].index[0]
    # print(index)
    similarity_scores = pd.Series(cosine_sim[index]).sort_values(ascending = False)
    top_10_movies = list(similarity_scores.iloc[1:11].index)
    recommended_movies = [list(finaldata.index)[i] for i in top_10_movies]
    print(recommended_movies)
    # return recommended_movies
  except:
    print("No movie name found")

recommendations("Deadpool")

recommendations("Ride Along 2")

recommendations("Spider-Man")

recommendations("Ice Age")

indices = pd.Series(finaldata.index)

print(indices[17])

def recommendations_query(query, dataframe = test):

  # Make new Dataframe
  temp = pd.DataFrame({"Title": ['Query'],
                     "Plot": [query]})
  
  test2 = test.append(temp)

  test2 = test2.set_index('Title')


  # Applying pre-processing
  test2["new_plot"] = test2["Plot"].apply(preprocess_sentences)

  # Make Tf-idf vectors
  tfidf = TfidfVectorizer()
  tfidf_movieid = tfidf.fit_transform((test2["new_plot"]))

  # Computing cosine similarity
  similarity = cosine_similarity(tfidf_movieid, tfidf_movieid)

  # Hashing on index
  indices = pd.Series(test2.index)

  try:
    index= indices[indices == 'Query'].index[0]
    # print(index)
    similarity_scores = pd.Series(similarity[index]).sort_values(ascending = False)
    top_10_movies = list(similarity_scores.iloc[1:11].index)
    recommended_movies = [list(finaldata.index)[i] for i in top_10_movies]
    # print(recommended_movies)
    return recommended_movies
  except:
    print("No movie name found")

recommendations_query("Army man in distress")

